{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kharga/Assignment/blob/main/worksheet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#.1.1 Step -1- Data Understanding, Analysis and Preparations:"
      ],
      "metadata": {
        "id": "QBOQ6E-CyDWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-do Task 1"
      ],
      "metadata": {
        "id": "MFDiuL_zyuNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvdkLOmUWi7i",
        "outputId": "dc194d47-031c-43d7-fbda-149f2119f2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Top 5 rows of data:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 rows of data:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "Information about the data:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n",
            "\n",
            "Description of the data:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n",
            "\n",
            "Features (X):\n",
            "   Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "\n",
            "Label (Y):\n",
            "0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Specify the path to your CSV file (Update this path to your file's actual location)\n",
        "file_path = '/content/drive/My Drive/student.csv'\n",
        "\n",
        "# 3. Read the CSV file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# 4. Display top and bottom rows of the data\n",
        "print(\"Top 5 rows of data:\")\n",
        "print(data.head())\n",
        "\n",
        "print(\"\\nBottom 5 rows of data:\")\n",
        "print(data.tail())\n",
        "\n",
        "# 5. Display information about the data\n",
        "print(\"\\nInformation about the data:\")\n",
        "print(data.info())\n",
        "\n",
        "# 6. Display descriptive statistics\n",
        "print(\"\\nDescription of the data:\")\n",
        "print(data.describe())\n",
        "\n",
        "# 7. Define features (X) and label (Y)\n",
        "X = data[['Math', 'Reading']]  # Features\n",
        "Y = data['Writing']  # Label (Output to be predicted)\n",
        "\n",
        "# Display features and label\n",
        "print(\"\\nFeatures (X):\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"\\nLabel (Y):\")\n",
        "print(Y.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-do task 2"
      ],
      "metadata": {
        "id": "hgaGNrIgyGo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X_matrix = X.T\n",
        "W = np.random.rand(X_matrix.shape[0], 1)\n",
        "Y_matrix = np.dot(W.T, X_matrix)\n",
        "\n",
        "print(\"\\nResulting Matrix (Y):\")\n",
        "print(Y_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4_5MVXDXN42",
        "outputId": "b1dd8f7f-a177-4cbb-d0b3-a57675d04b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resulting Matrix (Y):\n",
            "[[111.58848706 137.35072645 151.96792699 152.1917323  117.71774349\n",
            "  146.7569082  147.67514585  89.89522349 133.45397057 156.40991167\n",
            "  125.43665349 153.48297879  95.10624228 151.81872345 125.51125526\n",
            "   88.67857877 125.58585703  88.97698585 154.55041997 143.33077934\n",
            "  118.18837051 117.86694702 133.00635995 109.4766211  147.82434939\n",
            "  151.79570704 122.45813525 125.8842641  152.73696109 140.18004115\n",
            "  141.64350759 152.71394469 132.23732584 138.2919805   96.71891226\n",
            "  103.72037351  98.53237114 145.31645817 136.82851406  82.25091526\n",
            "  153.40837702 148.0711711  121.24149052  95.47925113 123.52557643\n",
            "  134.84283524  99.45060878 132.23732584 181.92532935 126.72789998\n",
            "  191.10770579 155.39405585 137.35072645  91.95550409  92.79913996\n",
            "   97.31572641 111.53690169  94.63561525 139.21021815 144.69662761\n",
            "  109.70042641 166.63671289 189.12202696 148.51878172  99.62282873\n",
            "  171.22790111 118.93438821 170.55648518 122.01052463 118.18837051\n",
            "  123.00336404 112.75354642  95.40464936 117.64314172 119.62882054\n",
            "   94.85942056 123.45097466  74.21058177 151.66951991 127.89295934\n",
            "  156.70831875 139.18720174 123.82398351 141.64350759  79.64540587\n",
            "  126.90011993 152.49013938 145.46566171 142.71094878 150.28065524\n",
            "  100.21964289 137.67214994 126.57869644 143.87600814 118.78518467\n",
            "  119.08359175 106.54968822 121.31609229 176.41590348 164.50183052\n",
            "   61.07986408  99.45060878 137.89595525  84.98261178  78.20495583\n",
            "  145.46566171 149.75844285  86.84210348 145.39105994  80.71284705\n",
            "  122.75654232 143.85299173 169.86205284 159.21620996  87.14051056\n",
            "  145.83867056 100.59265173 138.81419289 168.9438152  150.6766805\n",
            "  129.18420583 151.96792699 151.34809642 138.91181107  86.29687468\n",
            "   91.73169878  67.18610411 104.26560231 146.40691576 168.72000989\n",
            "  146.16009404 132.75953823 126.28028936 106.1020776  110.61866405\n",
            "  151.04968934 164.74865224 160.23206579 160.05984584 166.03989873\n",
            "  116.03047174 174.50482643 132.93175818 129.10960406 117.94154879\n",
            "  144.07679704 142.65936342 181.92532935 149.90764639 104.71321293\n",
            "  122.0851264  156.93212406 132.93175818 113.52258052 117.49393818\n",
            "   88.0587482  157.30513291 134.99203878 106.77349353 137.67214994\n",
            "  149.43701936 113.49956411  78.2795576  126.95170529 136.53010699\n",
            "  141.34510052 173.53500342 134.07380113 120.92006704 186.14350872\n",
            "  107.24412055 137.89595525 129.65483286 110.07343525 127.64613762\n",
            "  161.12728702 161.52331228 123.99620345 133.45397057 113.20115703\n",
            "  132.55874933 113.59718229 131.56590992 120.39785465 116.32887882\n",
            "  146.68230643 140.79987172 142.38952529 161.2249052  167.03273814\n",
            "  112.67894465 180.18647224 123.99620345 145.56327989  92.25391117\n",
            "  138.73959112 169.93665461 129.95323994 133.30476703 148.84020521\n",
            "  169.09301874 131.09528289 117.71774349 139.97925225 165.27086462\n",
            "   76.1962606  116.42649699  89.12618939 140.50146464 110.52104587\n",
            "  116.27729345 124.69063579 150.6766805   97.8379388  126.20568759\n",
            "  186.14350872  82.4001188  160.35825292 114.81382701 163.2851858\n",
            "  108.63298522 138.66498935 128.66199344 151.81872345 143.9275935\n",
            "  141.79271113 116.17967528 136.75391229 115.95586997 150.60207873\n",
            "   95.0086241  119.40501523 150.75128227 131.46829174 142.86015232\n",
            "  156.23769172 153.03536817  72.0756994  127.42233231 156.55911521\n",
            "  134.1484029   70.41144406 123.82398351  97.24112464 117.79234525\n",
            "  143.85299173  96.322887    60.31082998 120.39785465  76.74148939\n",
            "  171.37710465 122.15972817  93.94118292 147.74974762 151.57190173\n",
            "  143.70378819  69.17178293 131.46829174 140.50146464 123.15256758\n",
            "  122.38353348 122.30893171 106.49810285  36.3104641  122.30893171\n",
            "  171.54932459 133.60317411 124.66761938 140.3522611  109.37900292\n",
            "  187.13634814  88.97698585 128.04216288  74.90501411 154.72263992\n",
            "  116.50109876 112.75354642 156.55911521 139.97925225 127.19852701\n",
            "  135.68647111  87.73732472  92.87374173 129.65483286 138.73959112\n",
            "  159.83604053 139.583227    99.6974305   93.39595412 112.20831762\n",
            "  138.98641284 143.47998288 127.96756111 135.53726757 105.95287406\n",
            "  105.03463641  91.88090232  85.7746623  149.06401052 115.50825935\n",
            "  175.05005522 172.07153698 122.23432994  90.6642576  127.19852701\n",
            "  120.7938799  130.79687581  88.75318054 148.29497641 160.60507463\n",
            "  103.42196644 162.04552467 107.39332409 157.92496347 143.47998288\n",
            "  140.57606641 135.91027642 145.39105994 121.76370291 128.81119698\n",
            "  101.36168584 127.81835757 134.6936317  141.04669344 128.73659521\n",
            "  142.48714347 163.13598226 122.06210999 158.09718342 159.23922637\n",
            "  149.51162113 132.85715641 120.54705819  97.8379388  185.29987285\n",
            "  109.67741    135.68647111 124.83983933 167.55495053 102.42912702\n",
            "  157.47735285 110.9916729  181.32851519 119.15819352 103.72037351\n",
            "  155.41707226 181.70152404 127.42233231 124.14540699 148.98940875\n",
            "  130.84846117  68.42576524 168.84619702 126.82551816 171.07869757\n",
            "   64.05838232 145.09265286 155.09564877 112.82814819 139.0379982\n",
            "  150.50446055 167.72717048 106.8711117   91.20948639 117.27013287\n",
            "  126.67631462  89.89522349 143.25617757 128.04216288 133.70079228\n",
            "  167.18194168 131.64051169 129.10960406 148.98940875 136.53010699\n",
            "  106.54968822 117.79234525 137.59754817 154.02820759 170.63108695\n",
            "  164.27802521 165.66688988 129.2588076  146.60770466 143.95060991\n",
            "  130.64767227 131.24448643  98.83077822 127.34773054 159.61223522\n",
            "  116.57570053 130.5500541   87.83494289 157.70115816 108.78218876\n",
            "  101.58549115 147.92196756 131.46829174 153.48297879 125.28744995\n",
            "  183.76180464 118.7105829  137.82135348 113.12655526 133.47698697\n",
            "  152.04252876 137.82135348  99.00299816 118.01615056 119.40501523\n",
            "  173.06437639 183.16499048 120.62165996 149.21321406 128.58739168\n",
            "  113.37337698  77.43592173 151.96792699 177.95397169 137.67214994\n",
            "  108.68457058 108.60996881 148.0711711   97.16652287 132.33494402\n",
            "  180.85788817  88.43175705  72.84473351 138.49276941 169.09301874\n",
            "  146.77992461  79.12319348 153.20758812 158.32098873  97.91254057\n",
            "  135.06664055 160.75427817 168.84619702 134.6936317   78.20495583\n",
            "  159.9106423  156.78292052 110.39485874 177.87936992 130.40085056\n",
            "  151.04968934 163.06138049  99.82361763 140.8974899  118.26297228\n",
            "  104.88543288 132.55874933 138.51578581 112.82814819  87.90954466\n",
            "  155.09564877  84.23659409 162.66535523 133.23016526 144.42123693\n",
            "  117.64314172  82.92233119 102.75055051  98.08476052 130.5730705\n",
            "  144.15139881 129.2588076  149.83304462 119.62882054 160.90348171\n",
            "  135.31346226 150.6766805  141.0982788  129.35642578 146.08549228\n",
            "  149.5862229  130.5730705  113.14957167 120.32325288  99.67441409\n",
            "  155.71547933 111.83530877 138.44118404 134.6936317  133.52857234\n",
            "  151.81872345  90.81346114 149.21321406 101.13788053 143.95060991\n",
            "  152.9607664  127.64613762 114.51541993 177.25953936 165.12166108\n",
            "  129.08658766 131.78971523 153.55758056 102.05611817 104.19100054\n",
            "  128.26596819  77.13751465 114.5900217  113.2757588  109.00599407\n",
            "  129.8040364  133.84999582 156.31229349 145.46566171 138.66498935\n",
            "  142.80856695 108.78218876 102.50372879  84.53500117 150.82588403\n",
            "   77.5105235  122.01052463  83.39295821 155.86468287 106.79650993\n",
            "  153.03536817 126.35489113 122.77955873 147.00372992  91.18646998\n",
            "  120.09944757 146.23469581 150.60207873 137.44834463 107.78934935\n",
            "   99.00299816 112.05911408 135.88726001  57.1831082  186.51651757\n",
            "  126.4294929  154.4758182  102.42912702  88.90238408 158.54479404\n",
            "  121.01768521  82.17631349 127.59455226 157.92496347 150.82588403\n",
            "  152.93774999 160.85189635 173.58658878 154.02820759 146.60770466\n",
            "  133.99919936 104.41480585 174.05721581 119.92722762 130.02784171\n",
            "  118.48677759 142.56174524 143.87600814 124.07080522  89.67141818\n",
            "  129.2588076  151.81872345 180.26107401  86.5436964  149.06401052\n",
            "  153.18457171 115.35905581 165.04705931 124.07080522 169.07000233\n",
            "   93.09754704 123.37637289 139.83004871 138.59038758 138.81419289\n",
            "  117.09791292 117.09791292 109.77502817 129.10960406 137.22453932\n",
            "  152.63934292 116.57570053 135.38806403 104.34020408 155.09564877\n",
            "  136.53010699 120.99466881 144.32361876  83.01994936 129.03500229\n",
            "  113.89558937 191.10770579 112.15673226 136.15709814 114.96303055\n",
            "  116.87410761 157.62655639 171.07869757 189.12202696 133.4023852\n",
            "  128.56437527 151.42269819 144.86884755 176.88653051 118.56137936\n",
            "  125.6604588  127.19852701 115.8812682  117.86694702 138.2919805\n",
            "  149.13861229 176.88653051 135.91027642 166.28672045 102.97435582\n",
            "  141.12129521 154.87184346  74.30819995 133.08096172 125.58585703\n",
            "  132.01352053  68.50036701 138.19436233 104.041797   126.50409467\n",
            "  148.22037464 180.93248994 152.88616463 148.51878172 148.8171888\n",
            "   91.33567352 154.94644523 155.09564877 172.5191476  172.54216401\n",
            "  188.12918755 134.61902993 144.02521168 157.8503617  124.07080522\n",
            "  101.28708407 122.15972817 179.86504875 113.2757588  166.51052575\n",
            "  127.34773054 128.88579875 139.11259997 129.18420583 157.55195462\n",
            "  162.88916054 136.53010699 165.89069519 112.67894465 174.43022466\n",
            "  150.75128227 101.9585     160.08286225 123.92160168 146.53310289\n",
            "  121.93592286 116.87410761 184.68004228 132.85715641 143.33077934\n",
            "   89.96982526 114.71620884 173.98261404 117.1955311  115.8812682\n",
            "  108.33457814 179.17061642  74.53200526 164.27802521 149.13861229\n",
            "  114.88842878 113.74638583 139.43402346 124.59301761 144.32361876\n",
            "  167.65256871 151.34809642 101.28708407 123.89858528 182.84356699\n",
            "  136.45550522  95.7776582  175.59528402  91.65709701 141.02367703\n",
            "  191.10770579  73.98677646 179.26823459  95.38163295  92.12772404\n",
            "   53.90618288 117.79234525 106.32588291 118.7105829  141.94191467\n",
            "  100.14504112  87.36431587  72.0756994  174.90085168  69.17178293\n",
            "  111.14087644 149.73542644 111.06627467 144.4728223  187.28555167\n",
            "  118.63598113  66.56627354 175.59528402 153.18457171 109.22979938\n",
            "  133.99919936 150.9004858  139.28481992 154.32661466 127.04932347\n",
            "  150.82588403 172.61676578 113.8209876  155.86468287 161.59791405\n",
            "  108.31156174 157.67814175 118.7105829  133.3793688  135.76107288\n",
            "  138.11976056 110.61866405  81.25807585 153.7067841  155.17025054\n",
            "  165.5692717  133.3793688   75.30103936 135.21584409 109.62582464\n",
            "  124.9144411  151.12429111 156.85752229 184.68004228 120.39785465\n",
            "  188.12918755 161.2249052  136.30630168 121.71211755 150.60207873\n",
            "  171.30250288 125.6604588  137.22453932 113.20115703 160.30666755\n",
            "  138.59038758 134.99203878 128.19136642 158.54479404 130.27466342\n",
            "  131.26750284 159.06700642  96.91970116 132.46113115 134.44680998\n",
            "  134.37220821 129.95323994 132.38652938  96.62129408  30.80103824\n",
            "  111.06627467 170.38426523 145.09265286 131.864317   123.92160168\n",
            "  105.85525588 134.99203878 123.00336404 151.81872345 138.51578581\n",
            "   94.33720817 162.81455877 111.98451231 148.76560344 148.74258703\n",
            "  129.35642578  88.0587482   54.72680234 148.76560344 107.09491701\n",
            "  124.22000876 144.02521168 172.61676578 124.44381407 153.87900405\n",
            "  166.80893283 121.83830468 100.21964289 191.10770579 124.07080522\n",
            "  105.70605234 123.92160168 146.63072107 103.49656821 152.9607664\n",
            "   97.53953172 101.66009292 109.99883348 129.65483286  90.81346114\n",
            "  132.93175818 158.91780288 144.39822053  92.55231825 124.61603402\n",
            "  168.86921343 148.29497641 115.58286112 120.92006704 103.34736467\n",
            "  176.51352166 128.63897704 121.09228698 179.02141288 116.40348058\n",
            "  113.35036057 141.27049875 151.89332522 123.00336404 100.81645704\n",
            "  191.10770579 100.96566058 100.6672535  130.02784171 163.21058403\n",
            "  125.53427166 160.6796764  150.1314517  115.95586997 173.53500342\n",
            "  143.9275935  129.55721468 125.36205172 144.17441522  95.5538529\n",
            "   86.07306938 131.24448643  97.53953172 152.73696109 169.93665461\n",
            "   68.17894352 120.09944757 164.0542199  174.05721581 136.43248881\n",
            "  122.30893171 131.864317   151.04968934 106.8711117  122.01052463\n",
            "  134.22300467 157.47735285 125.98188228 125.36205172 130.94607935\n",
            "   76.89069293 140.20305756 120.54705819 106.25128114 147.92196756\n",
            "  115.50825935 147.00372992 156.63371698 133.15556349 112.82814819\n",
            "  139.50862523 185.59827993 171.30250288 158.14876878 125.73506057\n",
            "  118.56137936  95.94987815 111.38769815 115.3360394  160.6796764\n",
            "  115.95586997 126.20568759 143.25617757  89.74601995 167.40574699\n",
            "  120.0248458  159.46303168 127.42233231 137.37374286  81.55648292\n",
            "  122.38353348 128.96040052 127.42233231 148.44417995 115.28445404\n",
            "  144.17441522 132.31192761 160.60507463 101.13788053 176.19209818\n",
            "  184.53083874  86.76750171  81.70568646 169.46602759 160.00826048\n",
            "  142.03953285 104.41480585 156.16308995 152.73696109 129.72943463\n",
            "   85.47625522 117.64314172 120.32325288 129.95323994  86.59528176\n",
            "  126.05648405 110.37184233 156.01388641 147.302137   138.49276941\n",
            "  136.9777176  105.40764526 156.01388641 105.43066167 120.69626173\n",
            "  171.37710465 148.36957818  73.46456407 179.64124344  99.92123581\n",
            "  136.90311583 156.63371698 148.69100167 121.01768521 151.34809642\n",
            "  159.46303168 130.17704525 133.92459759 134.39522462 118.23995587\n",
            "  156.31229349 114.66462347  90.51505406 113.37337698 156.16308995\n",
            "   84.01278878 107.09491701 106.8480953  171.30250288 163.43438934\n",
            "  114.81382701 129.65483286 120.69626173 110.69326582 141.71810936\n",
            "  148.98940875 153.10996994 140.20305756 126.13108582 131.93891877\n",
            "  126.20568759 130.79687581 135.76107288 116.03047174 166.71131466\n",
            "  126.4294929  145.86168697 142.63634701  96.15066705 128.41517173\n",
            "  116.87410761 153.87900405 152.04252876 131.71511346 161.37410874\n",
            "  154.17741112  94.93402233 136.67931052 118.33757405 123.92160168\n",
            "  151.59491814 125.21284818 114.04479291 122.30893171 168.47318817\n",
            "  106.32588291 111.16389284 168.00256115 122.30893171 104.88543288\n",
            "  139.583227   152.41553761 168.10017933 157.62655639 126.13108582]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Split the dataset\n",
        "# Perform an 80-20 split (80% training, 20% testing)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Display the shapes of the resulting splits\n",
        "print(\"Training Feature Matrix (X_train):\", X_train.shape)\n",
        "print(\"Test Feature Matrix (X_test):\", X_test.shape)\n",
        "print(\"Training Label Vector (Y_train):\", Y_train.shape)\n",
        "print(\"Test Label Vector (Y_test):\", Y_test.shape)\n"
      ],
      "metadata": {
        "id": "CoEWe1q8YBL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1867f36d-77b1-4c97-b48a-38494d095df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Feature Matrix (X_train): (800, 2)\n",
            "Test Feature Matrix (X_test): (200, 2)\n",
            "Training Label Vector (Y_train): (800,)\n",
            "Test Label Vector (Y_test): (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1.2 Step -2- Build a Cost Function:"
      ],
      "metadata": {
        "id": "TRvbeQAy5PrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "    # Step 1: Compute predictions\n",
        "    Y_pred = np.dot(X, W)  # Shape: (n_samples,)\n",
        "\n",
        "    # Step 2: Compute errors (Y_pred - Y)\n",
        "    errors = Y_pred - Y  # Shape: (n_samples,)\n",
        "\n",
        "    # Step 3: Compute Mean Squared Error\n",
        "    n = X.shape[0]  # Number of training examples\n",
        "    cost = np.sum(errors**2) / (2 * n)  # MSE formula\n",
        "    return cost\n",
        "\n",
        "# Test data\n",
        "X_test = np.array([[1, 3, 5],  # Feature Matrix (rows = samples, cols = features)\n",
        "                   [2, 4, 6]]).T  # Transpose to match shape (3, 2)\n",
        "Y_test = np.array([3, 7, 11])  # Target Vector\n",
        "W_test = np.array([1, 1])  # Weight Vector\n",
        "\n",
        "# Compute cost\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "# Verify the output\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"Something went wrong: Reimplement the cost function\")\n",
        "print(\"Cost function output:\", cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRgqBYWM5V4s",
        "outputId": "65a128de-80be-4cb7-ff02-2b874efbcb5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1.3 Step -3- Gradient Descent for Simple Linear Regression:"
      ],
      "metadata": {
        "id": "Yoo5tWmJ84J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "  # Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    W_update = W\n",
        "# Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    W_update = W\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values (Y_pred = X · W)\n",
        "        Y_pred = np.dot(X, W_update)  # Shape: (m x 1)\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y (Loss)\n",
        "        loss = Y_pred - Y  # Shape: (m x 1)\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1 / m) * np.dot(X.T, loss)  # Shape: (n x 1)\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W_update = W_update - alpha * dw  # Shape: (n x 1)\n",
        "\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W_update)  # Compute the new cost\n",
        "        cost_history[iteration] = cost  # Store cost in history\n",
        "\n",
        "    return W_update, cost_history\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 3)\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3)\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlY0NKtX88-k",
        "outputId": "59f6555b-e415-460e-ca76-7368f4338eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "Cost History: [0.10711197094660153, 0.10634880599939901, 0.10559826315680616, 0.10486012948320558, 0.1041341956428534, 0.10342025583900626, 0.1027181077540776, 0.1020275524908062, 0.10134839451441931, 0.1006804415957737, 0.1000235047554587, 0.09937739820884377, 0.09874193931205609, 0.09811694850887098, 0.09750224927850094, 0.0968976680842672, 0.09630303432313951, 0.09571818027612913, 0.09514294105952065, 0.09457715457692842, 0.09402066147216397, 0.09347330508290015, 0.09293493139511913, 0.09240538899833017, 0.09188452904154543, 0.0913722051899995, 0.09086827358260123, 0.09037259279010502, 0.08988502377398917, 0.08940542984603007, 0.08893367662855953, 0.08846963201539432, 0.08801316613342668, 0.08756415130486386, 0.08712246201010665, 0.08668797485125507, 0.08626056851623205, 0.08584012374351278, 0.08542652328745133, 0.08501965188419301, 0.0846193962181636, 0.08422564488912489, 0.08383828837978763, 0.08345721902397185, 0.08308233097530582, 0.08271352017645425, 0.08235068432886682, 0.08199372286303816, 0.08164253690927113, 0.08129702926893387, 0.08095710438620353, 0.08062266832028739, 0.08029362871811391, 0.07996989478748553, 0.0796513772706855, 0.07933798841853087, 0.07902964196486459, 0.07872625310147845, 0.07842773845346054, 0.07813401605495937, 0.0778450053253578, 0.0775606270458499, 0.07728080333641404, 0.07700545763317514, 0.07673451466614989, 0.07646790043736812, 0.07620554219936448, 0.07594736843403344, 0.07569330883184205, 0.07544329427139428, 0.07519725679934072, 0.0749551296106282, 0.07471684702908327, 0.07448234448832412, 0.0742515585129952, 0.07402442670031911, 0.0738008877019607, 0.07358088120619749, 0.0733643479203919, 0.07315122955375959, 0.07294146880042966, 0.07273500932279067, 0.07253179573511871, 0.07233177358748233, 0.0721348893499193, 0.07194109039688139, 0.07175032499194182, 0.07156254227276149, 0.07137769223630935, 0.07119572572433286, 0.07101659440907385, 0.07084025077922623, 0.070666648126131, 0.07049574053020462, 0.07032748284759716, 0.07016183069707572, 0.0699987404471299, 0.06983816920329523, 0.06968007479569092, 0.06952441576676843, 0.06937115135926715, 0.06922024150437375, 0.06907164681008185, 0.06892532854974835, 0.0687812486508435, 0.06863936968389095, 0.06849965485159508, 0.06836206797815195, 0.06822657349874123, 0.06809313644919561, 0.067961722455845, 0.06783229772553254, 0.06770482903579932, 0.06757928372523506, 0.06745562968399212, 0.06733383534445969, 0.06721386967209597, 0.067095702156415, 0.06697930280212627, 0.06686464212042395, 0.06675169112042348, 0.0666404213007429, 0.06653080464122665, 0.06642281359480932, 0.06631642107951677, 0.06621160047060279, 0.06610832559281864, 0.06600657071281309, 0.0659063105316614, 0.06580752017752023, 0.06571017519840698, 0.06561425155510119, 0.06551972561416586, 0.06542657414108709, 0.06533477429352925, 0.06524430361470467, 0.06515514002685512, 0.06506726182484374, 0.06498064766985515, 0.06489527658320228, 0.06481112794023773, 0.06472818146436811, 0.0646464172211699, 0.06456581561260431, 0.06448635737133043, 0.0644080235551142, 0.06433079554133217, 0.06425465502156798, 0.06417958399630046, 0.06410556476968135, 0.06403257994440141, 0.0639606124166433, 0.06388964537111992, 0.06381966227619645, 0.06375064687909507, 0.06368258320118075, 0.06361545553332655, 0.06354924843135755, 0.06348394671157162, 0.06341953544633615, 0.06335599995975896, 0.06329332582343267, 0.06323149885225086, 0.06317050510029515, 0.06311033085679153, 0.06305096264213547, 0.06299238720398384, 0.0629345915134133, 0.06287756276114324, 0.06282128835382297, 0.0627657559103815, 0.06271095325843898, 0.06265686843077901, 0.06260348966188052, 0.06255080538450809, 0.06249880422636036, 0.06244747500677472, 0.06239680673348793, 0.06234678859945137, 0.06229740997970036, 0.06224866042827619, 0.06220052967520031, 0.062153007623499706, 0.062106084346282515, 0.06205975008386309, 0.06201399524093575, 0.06196881038379625, 0.061924186237610215, 0.061880113683727866, 0.0618365837570441, 0.06179358764340313, 0.061751116677047156, 0.06170916233810801, 0.0616677162501414, 0.06162677017770278, 0.061586316023964055, 0.0615463458283708, 0.06150685176433905, 0.06146782613699094, 0.0614292613809287, 0.061391150058046254, 0.06135348485537794, 0.06131625858298352, 0.061279464171868706, 0.06124309467194143, 0.061207143250002184, 0.0611716031877684, 0.06113646787993252, 0.061101730832252524, 0.06106738565967507, 0.06103342608449018, 0.06099984593451716, 0.06096663914132128, 0.0609337997384604, 0.0609013218597616, 0.06086919973762659, 0.06083742770136588, 0.06080600017556133, 0.06077491167845611, 0.06074415682037193, 0.060713730302153254, 0.060683626913637524, 0.060653841532151406, 0.060624369121032556, 0.0605952047281761, 0.06056634348460599, 0.060537780603070336, 0.06050951137666054, 0.0604815311774538, 0.060453835455178496, 0.06042641973590228, 0.06039927962074215, 0.060372410784596583, 0.060345808974898815, 0.06031947001039151, 0.06029338977992186, 0.06026756424125725, 0.060241989419920934, 0.06021666140804729, 0.0601915763632565, 0.06016673050754826, 0.060142120126214255, 0.06011774156676883, 0.06009359123789796, 0.06006966560842588, 0.06004596120629915, 0.060022474617588105, 0.059999202485504784, 0.059976141509438, 0.0599532884440042, 0.05993064009811483, 0.05990819333405906, 0.059885945066602345, 0.059863892262100066, 0.059842031937626106, 0.059820361160116395, 0.05979887704552664, 0.05977757675800453, 0.05975645750907579, 0.05973551655684408, 0.0597147512052044, 0.05969415880306974, 0.05967373674361096, 0.05965348246350928, 0.05963339344222168, 0.059613467201258485, 0.059593701303473294, 0.05957409335236496, 0.0595546409913911, 0.05953534190329372, 0.05951619380943562, 0.0594971944691485, 0.0594783416790919, 0.05945963327262296, 0.0594410671191769, 0.05942264112365792, 0.05940435322584049, 0.059386201399780576, 0.059368183653237094, 0.059350298027102844, 0.05933254259484532, 0.05931491546195686, 0.05929741476541398, 0.0592800386731462, 0.05926278538351338, 0.05924565312479226, 0.05922864015467153, 0.059211744759755505, 0.05919496525507604, 0.05917829998361292, 0.05916174731582211, 0.059145305649172315, 0.059128973407688926, 0.05911274904150609, 0.05909663102642617, 0.05908061786348662, 0.059064708078534194, 0.05904890022180654, 0.05903319286752055, 0.05901758461346795, 0.05900207408061755, 0.058986659912724324, 0.05897134077594504, 0.058956115358460404, 0.05894098237010357, 0.05892594054199501, 0.05891098862618344, 0.05889612539529293, 0.05888134964217588, 0.05886666017957195, 0.058852055839772675, 0.05883753547429179, 0.05882309795354117, 0.058808742166512155, 0.05879446702046235, 0.058780271440607684, 0.058766154369819606, 0.05875211476832761, 0.05873815161342641, 0.05872426389918856, 0.058710450636181515, 0.05869671085118971, 0.058683043586941104, 0.058669447901838714, 0.05865592286969638, 0.05864246757947903, 0.05862908113504752, 0.05861576265490756, 0.058602511271963004, 0.05858932613327336, 0.058576206399815284, 0.05856315124624814, 0.058550159860683564, 0.058537231444458875, 0.05852436521191438, 0.05851156039017436, 0.0584988162189318, 0.05848613195023677, 0.05847350684828838, 0.058460940189230176, 0.05844843126094919, 0.05843597936287806, 0.05842358380580092, 0.05841124391166213, 0.058398959013378576, 0.05838672845465502, 0.05837455158980245, 0.05836242778355972, 0.05835035641091811, 0.05833833685694878, 0.05832636851663321, 0.05831445079469654, 0.058302583105443666, 0.05829076487259809, 0.05827899552914358, 0.05826727451716844, 0.058255601287712386, 0.0582439753006161, 0.058232396024373266, 0.05822086293598511, 0.058209375520817355, 0.058197933272459756, 0.05818653569258779, 0.05817518229082684, 0.058163872584618616, 0.05815260609908985, 0.058141382366923164, 0.058130200928230166, 0.058119061330426776, 0.05810796312811039, 0.05809690588293942, 0.05808588916351474, 0.058074912545263056, 0.058063975610322414, 0.058053077947429504, 0.05804221915180897, 0.05803139882506447, 0.05802061657507169, 0.0580098720158732, 0.05799916476757483, 0.057988494456244134, 0.057977860713810364, 0.057967263177966154, 0.05795670149207094, 0.05794617530505593, 0.05793568427133074, 0.057925228050691516, 0.057914806308230836, 0.057904418714248757, 0.057894064944165734, 0.0578837446784368, 0.05787345760246728, 0.05786320340652982, 0.05785298178568306, 0.05784279243969133, 0.057832635072946, 0.05782250939438805, 0.0578124151174319, 0.05780235195989063, 0.057792319643902336, 0.05778231789585793, 0.0577723464463298, 0.05776240503000217, 0.05775249338560223, 0.05774261125583255, 0.05773275838730473, 0.05772293453047407, 0.057713139439575324, 0.057703372872559694, 0.05769363459103265, 0.057683924360193005, 0.05767424194877295, 0.05766458712897906, 0.05765495967643433, 0.057645359370121226, 0.05763578599232564, 0.057626239328581796, 0.05761671916761811, 0.057607225301304, 0.05759775752459752, 0.057588315635493874, 0.057578899434974906, 0.05756950872695933, 0.057560143318253806, 0.05755080301850501, 0.057541487640152184, 0.05753219699838088, 0.057522930911077144, 0.05751368919878268, 0.05750447168465076, 0.05749527819440267, 0.057486108556285255, 0.05747696260102877, 0.05746784016180581, 0.05745874107419066, 0.05744966517611951, 0.057440612307851226, 0.05743158231192885, 0.05742257503314173, 0.057413590318488285, 0.05740462801713937, 0.05739568798040233, 0.057386770061685605, 0.05737787411646401, 0.05736900000224435, 0.05736014757853204, 0.05735131670679789, 0.057342507250445686, 0.05733371907478018, 0.05732495204697581, 0.05731620603604562, 0.057307480912811105, 0.057298776549872255, 0.05729009282157824, 0.05728142960399854, 0.05727278677489465, 0.05726416421369212, 0.05725556180145319, 0.05724697942084986, 0.0572384169561373, 0.05722987429312795, 0.05722135131916572, 0.05721284792310102, 0.05720436399526589, 0.0571958994274496, 0.057187454112874896, 0.057179027946174334, 0.05717062082336728, 0.057162232641836994, 0.05715386330030848, 0.05714551269882637, 0.05713718073873334, 0.05712886732264895, 0.05712057235444866, 0.05711229573924336, 0.05710403738335914, 0.0570957971943175, 0.057087575080815786, 0.05707937095270803, 0.057071184720986066, 0.05706301629776107, 0.05705486559624517, 0.057046732530733744, 0.057038617016587564, 0.05703051897021566, 0.05702243830905817, 0.057014374951569684, 0.057006328817202634, 0.05699829982639134, 0.05699028790053585, 0.05698229296198645, 0.05697431493402824, 0.056966353740865984, 0.05695840930760929, 0.056950481560257914, 0.0569425704256875, 0.0569346758316353, 0.05692679770668645, 0.05691893598026014, 0.05691109058259633, 0.05690326144474244, 0.05689544849854041, 0.056887651676613984, 0.05687987091235604, 0.056872106139916355, 0.05686435729418944, 0.05685662431080263, 0.0568489071261043, 0.05684120567715238, 0.056833519901703065, 0.05682584973819958, 0.05681819512576124, 0.05681055600417275, 0.056802932313873525, 0.056795323995947306, 0.056787730992111936, 0.05678015324470926, 0.05677259069669528, 0.05676504329163035, 0.05675751097366966, 0.05674999368755382, 0.05674249137859953, 0.05673500399269066, 0.05672753147626912, 0.056720073776326194, 0.05671263084039382, 0.056705202616536096, 0.05669778905334098, 0.05669039009991206, 0.05668300570586036, 0.05667563582129657, 0.056668280396823104, 0.05666093938352648, 0.05665361273296975, 0.056646300397185066, 0.05663900232866641, 0.05663171848036241, 0.05662444880566922, 0.05661719325842369, 0.056609951792896414, 0.056602724363785134, 0.05659551092620811, 0.05658831143569758, 0.05658112584819342, 0.05657395412003692, 0.05656679620796451, 0.05655965206910183, 0.05655252166095763, 0.056545404941418, 0.056538301868740586, 0.05653121240154893, 0.056524136498826864, 0.056517074119913024, 0.056510025224495546, 0.05650298977260663, 0.05649596772461748, 0.056488959041233064, 0.05648196368348717, 0.05647498161273735, 0.05646801279066009, 0.056461057179246134, 0.05645411474079556, 0.05644718543791332, 0.05644026923350467, 0.056433366090770515, 0.05642647597320331, 0.05641959884458242, 0.05641273466897008, 0.05640588341070717, 0.05639904503440896, 0.05639221950496121, 0.05638540678751615, 0.05637860684748858, 0.056371819650551894, 0.05636504516263454, 0.05635828334991602, 0.05635153417882347, 0.056344797616027884, 0.056338073628440656, 0.05633136218321008, 0.056324663247717996, 0.05631797678957624, 0.05631130277662355, 0.05630464117692215, 0.056297991958754665, 0.05629135509062088, 0.05628473054123466, 0.05627811827952098, 0.05627151827461283, 0.0562649304958483, 0.05625835491276777, 0.05625179149511093, 0.056245240212814004, 0.05623870103600713, 0.05623217393501148, 0.05622565888033667, 0.05621915584267811, 0.05621266479291449, 0.056206185702105234, 0.05619971854148787, 0.05619326328247578, 0.05618681989665565, 0.05618038835578517, 0.056173968631790624, 0.05616756069676472, 0.056161164522964185, 0.05615478008280768, 0.05614840734887347, 0.05614204629389748, 0.056135696890770956, 0.0561293591125386, 0.056123032932396344, 0.05611671832368947, 0.05611041525991056, 0.05610412371469757, 0.05609784366183199, 0.05609157507523687, 0.05608531792897493, 0.05607907219724691, 0.056072837854389615, 0.05606661487487427, 0.056060403233304724, 0.056054202904415734, 0.05604801386307135, 0.056041836084263226, 0.05603566954310899, 0.05602951421485069, 0.05602337007485318, 0.05601723709860265, 0.05601111526170498, 0.05600500453988435, 0.05599890490898177, 0.05599281634495359, 0.055986738823870105, 0.055980672321914095, 0.055974616815379595, 0.055968572280670384, 0.05596253869429871, 0.05595651603288404, 0.05595050427315166, 0.0559445033919315, 0.05593851336615684, 0.055932534172863126, 0.05592656578918666, 0.05592060819236353, 0.05591466135972839, 0.05590872526871329, 0.05590279989684662, 0.05589688522175184, 0.055890981221146614, 0.05588508787284151, 0.055879205154739084, 0.05587333304483278, 0.05586747152120588, 0.055861620562030555, 0.05585578014556684, 0.05584995025016162, 0.05584413085424776, 0.05583832193634303, 0.055832523475049294, 0.0558267354490515, 0.05582095783711688, 0.05581519061809395, 0.05580943377091164, 0.05580368727457861, 0.05579795110818212, 0.055792225250887444, 0.055786509681936956, 0.055780804380649245, 0.05577510932641847, 0.05576942449871345, 0.055763749877076975, 0.05575808544112503, 0.05575243117054599, 0.05574678704509999, 0.05574115304461813, 0.055735529149001775, 0.055729915338221844, 0.05572431159231819, 0.05571871789139883, 0.05571313421563932, 0.05570756054528211, 0.05570199686063586, 0.055696443142074864, 0.055690899370038335, 0.05568536552502988, 0.05567984158761686, 0.055674327538429685, 0.05566882335816142, 0.055663329027567085, 0.055657844527463085, 0.05565236983872667, 0.05564690494229538, 0.0556414498191665, 0.05563600445039652, 0.055630568817100635, 0.05562514290045211, 0.05561972668168197, 0.05561432014207832, 0.05560892326298588, 0.05560353602580557, 0.055598158411993996, 0.055592790403062906, 0.055587431980578784, 0.055582083126162425, 0.05557674382148841, 0.055571414048284674, 0.05556609378833211, 0.055560783023464094, 0.05555548173556606, 0.0555501899065751, 0.05554490751847952, 0.05553963455331848, 0.05553437099318152, 0.055529116820208266, 0.05552387201658792, 0.055518636564558965, 0.05551341044640875, 0.05550819364447313, 0.05550298614113609, 0.05549778791882936, 0.05549259896003212, 0.05548741924727061, 0.05548224876311773, 0.055477087490192804, 0.0554719354111612, 0.055466792508733924, 0.05546165876566746, 0.055456534164763226, 0.05545141868886745, 0.05544631232087083, 0.05544121504370806, 0.055436126840357744, 0.055431047693841926, 0.05542597758722593, 0.055420916503617974, 0.05541586442616892, 0.055410821338071965, 0.05540578722256242, 0.05540076206291734, 0.055395745842455345, 0.05539073854453634, 0.055385740152561175, 0.05538075064997147, 0.055375770020249314, 0.05537079824691705, 0.055365835313536935, 0.05536088120371103, 0.05535593590108084, 0.055350999389327124, 0.055346071652169704, 0.0553411526733671, 0.05533624243671647, 0.05533134092605322, 0.055326448125250914, 0.05532156401822096, 0.05531668858891247, 0.055311821821311946, 0.055306963699443185, 0.05530211420736701, 0.055297273329180996, 0.05529244104901939, 0.0552876173510529, 0.05528280221948834, 0.05527799563856866, 0.055273197592572584, 0.05526840806581448, 0.055263627042644155, 0.055258854507446706, 0.05525409044464235, 0.05524933483868613, 0.05524458767406786, 0.05523984893531189, 0.055235118606976955, 0.05523039667365595, 0.05522568311997588, 0.055220977930597534, 0.05521628109021546, 0.05521159258355772, 0.055206912395385714, 0.055202240510494154, 0.05519757691371069, 0.055192921589895944, 0.05518827452394329, 0.05518363570077869, 0.05517900510536053, 0.05517438272267951, 0.055169768537758505, 0.055165162535652366, 0.055160564701447826, 0.05515597502026334, 0.055151393477248956, 0.05514682005758613, 0.055142254746487665, 0.055137697529197546, 0.0551331483909908, 0.055128607317173346, 0.055124074293081866, 0.055119549304083755, 0.05511503233557687, 0.05511052337298952, 0.05510602240178027, 0.05510152940743782, 0.05509704437548093, 0.055092567291458276, 0.05508809814094826, 0.05508363690955906, 0.055079183582928334, 0.05507473814672324, 0.055070300586640204, 0.05506587088840496, 0.05506144903777224, 0.05505703502052585, 0.055052628822478474, 0.05504823042947156, 0.05504383982737522, 0.05503945700208816, 0.05503508193953754, 0.055030714625678864, 0.05502635504649593, 0.05502200318800065, 0.055017659036233034, 0.055013322577261034, 0.055008993797180404, 0.05500467268211479, 0.05500035921821539, 0.054996053391661005, 0.05499175518865794, 0.054987464595439836, 0.054983181598267664, 0.0549789061834296, 0.05497463833724084, 0.05497037804604376, 0.0549661252962075, 0.054961880074128146, 0.05495764236622849, 0.054953412158958034, 0.054949189438792796, 0.05494497419223534, 0.05494076640581462, 0.05493656606608597, 0.05493237315963089, 0.05492818767305708, 0.05492400959299837, 0.05491983890611449, 0.054915675599091204, 0.05491151965864005, 0.05490737107149833, 0.05490322982442909, 0.054899095904220915, 0.05489496929768798, 0.05489084999166989, 0.05488673797303166, 0.054882633228663574, 0.054878535745481176, 0.054874445510425175, 0.05487036251046136, 0.054866286732580524, 0.054862218163798465, 0.05485815679115577, 0.0548541026017179, 0.054850055582575, 0.05484601572084191, 0.0548419830036581, 0.054837957418187463, 0.05483393895161846, 0.0548299275911639, 0.054825923324060916, 0.05482192613757092, 0.05481793601897949, 0.05481395295559637, 0.05480997693475535, 0.05480600794381422, 0.05480204597015469, 0.05479809100118241, 0.05479414302432678, 0.054790202027040935, 0.05478626799680178, 0.05478234092110976, 0.05477842078748891, 0.0547745075834868, 0.054770601296674395, 0.05476670191464608, 0.05476280942501957, 0.054758923815435796, 0.05475504507355896, 0.05475117318707636, 0.0547473081436984, 0.05474344993115854, 0.0547395985372132, 0.054735753949641676, 0.0547319161562462, 0.05472808514485178, 0.054724260903306156, 0.05472044341947975, 0.054716632681265726, 0.05471282867657969, 0.0547090313933599, 0.054705240819567, 0.05470145694318413, 0.05469767975221677, 0.05469390923469267, 0.05469014537866194, 0.05468638817219682, 0.05468263760339178, 0.05467889366036329, 0.05467515633125, 0.05467142560421251, 0.05466770146743334, 0.054663983909116975, 0.054660272917489705, 0.05465656848079964, 0.05465287058731666, 0.05464917922533233, 0.05464549438315984, 0.054641816049134054, 0.054638144211611325, 0.05463447885896954, 0.05463081997960807, 0.05462716756194763, 0.05462352159443038, 0.054619882065519716, 0.054616248963700355, 0.05461262227747823, 0.05460900199538041, 0.054605388105955124, 0.054601780597771675, 0.05459817945942039, 0.05459458467951261, 0.05459099624668059, 0.05458741414957748, 0.0545838383768773, 0.054580268917274875, 0.05457670575948582, 0.05457314889224635, 0.054569598304313474, 0.0545660539844648, 0.054562515921498494, 0.05455898410423328, 0.054555458521508345, 0.05455193916218337, 0.05454842601513845, 0.05454491906927401, 0.05454141831351079, 0.054537923736789846, 0.054534435328072464, 0.0545309530763401, 0.054527476970594374, 0.05452400699985704, 0.05452054315316988, 0.05451708541959473, 0.054513633788213396, 0.054510188248127645, 0.05450674878845912, 0.05450331539834934, 0.054499888066959656, 0.05449646678347117, 0.054493051537084725, 0.05448964231702087, 0.05448623911251984, 0.05448284191284145, 0.054479450707265106, 0.05447606548508972, 0.054472686235633755, 0.054469312948235114, 0.0544659456122511, 0.05446258421705838, 0.05445922875205301, 0.05445587920665035, 0.05445253557028493, 0.05444919783241064, 0.05444586598250043, 0.054442540010046496, 0.05443921990456002, 0.0544359056555714, 0.054432597252629965, 0.05442929468530409, 0.054425997943181044, 0.05442270701586707, 0.05441942189298729, 0.05441614256418564, 0.05441286901912488, 0.05440960124748651, 0.054406339238970806, 0.05440308298329671, 0.054399832470201845, 0.054396587689442416, 0.05439334863079324, 0.05439011528404767, 0.054386887639017584, 0.054383665685533336, 0.0543804494134437, 0.054377238812615865, 0.05437403387293539, 0.054370834584306166, 0.05436764093665037, 0.054364452919908414, 0.05436127052403898, 0.05435809373901896, 0.05435492255484332]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1.4 Step -4- Evaluate the Model:"
      ],
      "metadata": {
        "id": "M0UOykJFB5rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(Y, Y_pred):\n",
        "    squared_diff = (Y - Y_pred) ** 2\n",
        "\n",
        "    # Taking the mean of the squared differences\n",
        "    mean_squared_diff = np.mean(squared_diff)\n",
        "\n",
        "    # Taking the square root of the mean squared difference\n",
        "    rmse = np.sqrt(mean_squared_diff)\n",
        "\n",
        "    return rmse\n",
        "\n",
        "def r2(Y, Y_pred):\n",
        "    mean_y = np.mean(Y)\n",
        "\n",
        "    # Total Sum of Squares (SST)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "\n",
        "    # Sum of Squared Residuals (SSR)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "\n",
        "    # R-squared calculation\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "    return r2\n"
      ],
      "metadata": {
        "id": "VUR5fUjrB7Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1.5 Step -5- Main Function to Integrate All Steps:"
      ],
      "metadata": {
        "id": "0nj6yqIrCeU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    # 1. Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # 2. Specify the path to your CSV file (Update this path to your file's actual location)\n",
        "    file_path = '/content/drive/My Drive/student.csv'\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values  # Target: Writing marks\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights (for 2 features: Math and Reading)\n",
        "    alpha = 0.00001  # Learning rate\n",
        "    iterations = 1000  # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)  # Linear model prediction\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f77SVPE2CfYa",
        "outputId": "2294d874-5046-4fac-9dfd-d80814e36e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10 iterations): [2013.165570783755, 1640.286832599692, 1337.0619994901585, 1090.4794892850578, 889.9583270083234, 726.8940993009545, 594.2897260808594, 486.4552052951635, 398.7634463599484, 327.4517147324688]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Did your Model Overfitt, Underfitts, or performance is acceptable.\n",
        "\n",
        "=> My model underfitts since RMSE is much higher and r2 is low\n",
        "\n",
        "2. Experiment with different value of learning rate, making it higher and lower, observe the result.\n",
        "\n",
        "=> Lower learning rates gradually decreases the cost and slows convergence but could result in more stable and accurate results. However, higher learning rates may cause the model to converge too quicky."
      ],
      "metadata": {
        "id": "P4XHbDZsGBBr"
      }
    }
  ]
}